{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to read the data. I'll load the HEF and header data into pandas dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ntplib import system_to_ntp_time\n",
    "\n",
    "NTP_DIFF = system_to_ntp_time(0)\n",
    "\n",
    "hef_file = os.path.join('data', 'horizontal_electric_field.nc')\n",
    "header_file = os.path.join('data', 'hpies_data_header.nc')\n",
    "\n",
    "def netcdf_to_pandas(path):\n",
    "    h5file = h5py.File(path)\n",
    "    times = h5file['time'].value\n",
    "    times = times - NTP_DIFF\n",
    "    times = pd.to_datetime(times, unit='s')\n",
    "    data = {}\n",
    "    for group in h5file:\n",
    "        if group == 'time':\n",
    "            continue\n",
    "        for var in h5file[group]:\n",
    "            if 'timestamp' in var:\n",
    "                continue\n",
    "            data[var] = h5file[group][var].value\n",
    "    return pd.DataFrame(data, index=times)\n",
    "    \n",
    "hef_df = netcdf_to_pandas(hef_file)\n",
    "header_df = netcdf_to_pandas(header_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can check the contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12009\n",
      "252\n"
     ]
    }
   ],
   "source": [
    "print len(hef_df)\n",
    "print len(header_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have lots of HEF data and header data at around 1/50th the data rate. Let's re-index the header data to the same time series as the HEF data, forward filling the missing data points (using the last seen value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12009\n"
     ]
    }
   ],
   "source": [
    "header_df = header_df.reindex(index=hef_df.index, method='ffill')\n",
    "print len(header_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the time series for both datasets match, let's join the two dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "joined = hef_df.join(header_df, rsuffix='_header')\n",
    "print len(joined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now joined contains the combination of the HEF data and header data. We now have a hpies_hcno value for all data points in the HEF data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-04-29 14:21:08.658715       0\n",
      "2015-04-29 14:21:08.708307       0\n",
      "2015-04-29 14:21:08.766100       0\n",
      "2015-04-29 14:21:08.825862       0\n",
      "2015-04-29 14:21:08.887132       0\n",
      "2015-04-29 14:21:08.947684       0\n",
      "2015-04-29 14:21:09.009053       0\n",
      "2015-04-29 14:21:09.069960       0\n",
      "2015-04-29 14:21:09.131373999    0\n",
      "2015-04-29 14:21:09.191834       0\n",
      "2015-04-29 14:21:09.253042       0\n",
      "2015-04-29 14:21:09.318136       0\n",
      "2015-04-29 14:21:09.379428       0\n",
      "2015-04-29 14:21:09.440475       0\n",
      "2015-04-29 14:21:09.505502       0\n",
      "2015-04-29 14:21:09.566555       0\n",
      "2015-04-29 14:21:09.627611       0\n",
      "2015-04-29 14:21:09.692813       0\n",
      "2015-04-29 14:21:09.753664       0\n",
      "2015-04-29 14:21:09.818702       0\n",
      "Name: hpies_hcno, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print joined.hpies_hcno.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the data rate for the HEF data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data rate: 16.32 Hz\n"
     ]
    }
   ],
   "source": [
    "data_rate = 1 / np.median(np.diff(joined.index.values).astype('f64') / 1e9)\n",
    "print 'data rate: %.2f Hz' % data_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's compute the storage requirements for the lifetime of the project if we are to add just one single 32-bit integer to each HEF particle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected storage cost for adding one int to each particle: 51.48 GB\n"
     ]
    }
   ],
   "source": [
    "one_day = data_rate * 60 * 60 * 24\n",
    "one_year = one_day * 365\n",
    "project_lifetime = one_year * 25\n",
    "int_storage = project_lifetime * 4 / 1e9\n",
    "print 'Expected storage cost for adding one int to each particle: %.2f GB' % int_storage "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, by storing the header data separately and putting the onus on the user to combine the datasets we save over 51 GB (just for the hcno data alone.)\n",
    "\n",
    "Now, let's see if we can figure out what went wrong to produce the occasional jump in eindex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import ntplib\n",
    "import struct\n",
    "import time\n",
    "import os\n",
    "\n",
    "message_types = ( 'Data From Instrument',\n",
    "                  'Data From Driver',\n",
    "                  'Port Agent Command',\n",
    "                  'Port Agent Status',\n",
    "                  'Port Agent Fault',\n",
    "                  'Instrument Command',\n",
    "                  'Heartbeat',\n",
    "                  'Pickled Data From Instrument',\n",
    "                  'Pickled Data From Driver' )\n",
    "\n",
    "HEADER_FORMAT = '>3sBHHII'\n",
    "SYNC_BYTES = '\\xa3\\x9d\\x7a'\n",
    "raw_data_fh = open(os.path.join('data', 'hpies-port_agent.20150429.data'))\n",
    "raw_data = raw_data_fh.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ntp_time_to_ascii(tstamp):\n",
    "    return time.asctime(time.gmtime(ntplib.ntp_to_system_time(tstamp)))\n",
    "\n",
    "def parse_header(data):\n",
    "    sync, message_type, packet_size, checksum, time_upper, time_lower = struct.unpack_from(HEADER_FORMAT, data)\n",
    "    timestamp = time_upper + float(time_lower) / 2**32\n",
    "    return message_type, packet_size, checksum, timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data from the instrument is message type 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HEADER_SIZE = struct.calcsize(HEADER_FORMAT)\n",
    "packets = []\n",
    "while raw_data:\n",
    "    start = raw_data.find(SYNC_BYTES)\n",
    "    if start == -1:\n",
    "        break\n",
    "    \n",
    "    message_type, packet_size, checksum, timestamp = parse_header(raw_data[start:])\n",
    "    packet = raw_data[start:start+packet_size]\n",
    "    raw_data = raw_data[start+packet_size:]\n",
    "    \n",
    "    if message_type == 1:\n",
    "        packets.append(packet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
